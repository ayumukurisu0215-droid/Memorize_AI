import os
import time

# --- 1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate

# --- 2. è¨­å®š (APIã‚­ãƒ¼ã‚’ã“ã“ã«å…¥åŠ›) ---

# --- 3. è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ (ChromaDB + Gemini) ---

def setup_memory_ai():
    print("ğŸ§  è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ (ChromaDB)ã‚’èµ·å‹•ä¸­...")

    # A. åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
    # è¨€è‘‰ã‚’æ•°å€¤(ãƒ™ã‚¯ãƒˆãƒ«)ã«å¤‰æ›ã™ã‚‹ç¿»è¨³æ©Ÿã§ã™
    embeddings = GoogleGenerativeAIEmbeddings(model="gemini-embedding-001")

    # B. ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹(ChromaDB)ã®æº–å‚™
    # "./chroma_memory_db" ã¨ã„ã†ãƒ•ã‚©ãƒ«ãƒ€ã«è¨˜æ†¶ã‚’æ°¸ç¶šä¿å­˜ã—ã¾ã™
    vectorstore = Chroma(
        collection_name="chat_history",
        embedding_function=embeddings,
        persist_directory="./chroma_memory_db"  # ã“ã“ã«ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã•ã‚Œã‚‹
    )

    # C. AIãƒ¢ãƒ‡ãƒ«(Gemini)ã®æº–å‚™
    llm = ChatGoogleGenerativeAI(model="models/gemini-2.5-flash")

    return vectorstore, llm

# --- 4. ä¼šè©±ã®å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ ---

def chat_with_memory(user_input, vectorstore, llm):
    # A. éå»ã®è¨˜æ†¶ã‚’æ¤œç´¢ (Retrieve)
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã«ã€Œæ„å‘³ãŒè¿‘ã„ã€éå»ãƒ­ã‚°ã‚’3ä»¶æ¢ã—ã¦ãã‚‹
    search_results = vectorstore.similarity_search(user_input, k=3)
    
    # æ¤œç´¢çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆã«ã¾ã¨ã‚ã‚‹
    context_text = "\n".join([doc.page_content for doc in search_results])
    
    if not context_text:
        context_text = "ï¼ˆéå»ã®é–¢é€£ã™ã‚‹ä¼šè©±ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰"

    # B. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
    # AIã«ã€Œéå»ã®è¨˜æ†¶ã€ã¨ã€Œä»Šã®ç™ºè¨€ã€ã‚’åŒæ™‚ã«æ¸¡ã™
    template = """
    ã‚ãªãŸã¯è¦ªã—ã„å‹äººAIã§ã™ã€‚ä»¥ä¸‹ã®ã€éå»ã®è¨˜æ†¶ã€‘ã‚’ãƒ’ãƒ³ãƒˆã«ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
    
    ã€éå»ã®è¨˜æ†¶ã€‘
    {context}
    
    ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»Šã®ç™ºè¨€ã€‘
    {input}
    
    å›ç­”:
    """
    
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | llm

    # C. AIã‹ã‚‰ã®å›ç­”ã‚’ç”Ÿæˆ
    response = chain.invoke({"context": context_text, "input": user_input})
    ai_message = response.content

    # D. ä»Šã®ä¼šè©±ã‚’è¨˜æ†¶ã«ä¿å­˜ (Save)
    # ä¼šè©±ã®å†…å®¹ã‚’ChromaDBã«è¿½åŠ ã™ã‚‹
    # "User: ... / AI: ..." ã¨ã„ã†å½¢å¼ã§ä¿å­˜ã—ã¦ãŠãã¨æ–‡è„ˆãŒåˆ†ã‹ã‚Šã‚„ã™ã„
    memory_text = f"User: {user_input} / AI: {ai_message}"
    vectorstore.add_documents([Document(page_content=memory_text)])

    return ai_message, context_text

# --- 5. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ«ãƒ¼ãƒ— ---

if __name__ == "__main__":
    # ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    vectorstore, llm = setup_memory_ai()
    print("ğŸ¤– æº–å‚™å®Œäº†ï¼ä¼šè©±ã‚’å§‹ã‚ã¾ã—ã‚‡ã† (çµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¨å…¥åŠ›)")
    print("-" * 50)

    while True:
        user_input = input("ã‚ãªãŸ: ")
        
        if user_input.lower() == "exit":
            print("ã¾ãŸã­ï¼(è¨˜æ†¶ã¯ä¿å­˜ã•ã‚Œã¾ã—ãŸ)")
            break
            
        # AIã¨ä¼šè©±
        response, context = chat_with_memory(user_input, vectorstore, llm)
        
        print(f"AI: {response}")
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨: AIãŒä½•ã‚’æ€ã„å‡ºã—ã¦ã„ãŸã‹ã‚’è¡¨ç¤º
        # print(f"\n[æ€ã„å‡ºã—ã¦ã„ãŸã“ã¨]:\n{context}\n")
        print("-" * 50)